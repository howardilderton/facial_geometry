<!DOCTYPE HTML>
<!--
	Overflow by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Facial Geometry - A datascience project</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body>
	
		<!-- Header -->	
			<section id="header">
				<header>				
					<span class="image"><img src="images/FrontView2.png"></span>				
					<h1>Facial Geometry - A datascience project</h1>
					<p>By Howard Ilderton</p>
				</header>
				<footer>
					<a href="#banner" class="button style2 scrolly-middle">TO THE SCIENCE!</a>
				</footer>
			</section>

		<!-- Banner -->
			<section id="banner">
				<header>
					<h2>Introduction</h2>
				</header>
				<p>This is fun datascience project about detecting faces in images and calculating facial geometry.<br />
				We then use these geometric features to solve several classification problems:
				</p><br />
				<p>
				1. Facial recognition<br />
				2. Gender classification<br />
				3. Race classification<br />
				4. Age regression<br />
				5. Attractiveness regression
				</p><br />
			</section>
			
		<!-- Facial Geometry -->	
			<article class="container box style3">
				<header>
					<h2>What is facial geometry</h2>
				</header>
				<section>
					<p>For this project we are specifically interested in the ratios of distances between different landmarks on a face. We therefore don't care about eye color, skin color,
					hairstyle, and any other features that we humans can see in the image.
					<br />
					<br />
					But what exactly do I mean by "ratios of distances between different landmarks"?
					<br />
					<br />
					</p>
				</section>
			</article>			
					
		<!-- Facial Ratios -->
			<article id="first" class="container box style1 right">
				<span class="image fit"><img src="images/ratios.png"></span>
				<div class="inner">
					<header>
						<h2>Facial Ratios</h2>
					</header>
					<p>To calculate a facial ratio we need the distances between two pairs of points.<br />
					For example <font color="red">(A to B) : (C to D)</font><br />
					<br />
					<img src="https://latex.codecogs.com/gif.latex?\\&space;points&space;(x_{n},&space;y_{n})&space;\\&space;\\&space;distance1&space;=&space;\sqrt{(x_{1}-x_{2})
					^2&space;&plus;&space;(y_{1}-y_{2})^2}&space;\\&space;\\&space;distance2&space;=&space;\sqrt{(x_{3}-x_{4})^2&space;&plus;&space;(y_{3}-y_{4})^2}&space;\\&space;
					\\&space;ratio&space;=&space;\frac{distance1}{distance2}" title="\\ points (x_{n}, y_{n}) \\ \\ distance1 = \sqrt{(x_{1}-x_{2})^2 + (y_{1}-y_{2})^2} \\ \\ distance2
					= \sqrt{(x_{3}-x_{4})^2 + (y_{3}-y_{4})^2} \\ \\ ratio = \frac{distance1}{distance2}" /><br />
					</p>
				</div>
			</article>
						
		<!-- Approach -->	
			<article class="container box style3">
				<header>
					<h2>Approach</h2>
				</header>
				<section>
					<p>In order to build up our dataset of ratios there are several steps we'll need to follow:
					<br />
					</p>
					<ol class="default">
						<li>Identify <strong>faces</strong> within images</li>
						<li>Identify <strong>landmarks</strong> within faces</li>
						<li>Calculate <strong>distances</strong> and <strong>ratios</strong> between key landmarks</li>
						<li>Reduce large number of dimensions via <strong>Principle Component Analysis</strong></li>	
					</ol>
				</section>
			</article>	

		<!-- Identify Faces -->
			<article id="second" class="container box style1 left">
				<a href="#" class="image fit"><img src="images/S638MH.gif" alt="" /></a>
				<div class="inner">
					<header>
						<h2>1. Identify Faces</h2>
					</header>
					<p>In order to identify faces in our images we will use the <a href="http://opencv.org/" target="_blank">OpenCV</a> library which
					includes a trained <strong>Haar Cascade Classifier</strong>.
					<br />
					<br />
					OpenCV includes a basic tutorial and <a href="http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html" target="_blank">sample code here</a>. 
					</p>
				</div>
			</article>
		
		<!-- Haar Features -->	
			<article class="container box style3">
				<header>
					<h2>Haar Cascade Classifier</h2>
				</header>
				<section>
					<h3>What are Haar-like features</h3>
					<p>The popular Viola-Jones face detection algorithm uses rectangular features, known as <a href="https://en.wikipedia.org/wiki/Haar-like_features" target="_blank">Haar-like features</a>, 
					as shown in the figure below. These features are calculated as the sum-of-pixels within clear rectangles subtracted from the sum-of-pixels within shaded rectangles, and are used to find properties in an 
					image that are similar to regions of a face, for example the eyes region is darker than the upper-cheeks (Feature B) and the nose bridge region is brighter than the eyes (Feature C). 
					By operating at this region-level of granularity, Haar-like features are effective in finding faces irrespective of variations in facial expressions.
					<br />
					<br />
					<span class="image fit"><img src="images/haar.png"></span>
					<br />
					<h3>What is a cascade classifier</h3>
					The <a href="https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework" target="_blank">Violaâ€“Jones object detection framework</a>
					uses a learning algorithm to select the best face features and trains classifiers that use them. The matching algorithm moves a sliding window over an image using Haar-like features 
					to detect possible matches. Haar-like features are weak classifiers that are very inaccurate in isolation, so Viola-Jones uses a linear combination of weighted weak classifiers, 
					known as a boosted classifier, which balances performance and accuracy by allowing the algorithm to reject non-face images quickly while detecting faces with high probability. 
					As shown in the diagram below, at each stage of the cascade the image in the current window is tested against a small number of features: those that fail are rejected and those that pass 
					progress to the next stage. The classifier trained by Viola and Jones uses a 38-stage cascade of gradually increasing complexity. The first classifier is a simple attentional operator that uses 
					two features to achieve a false negative rate of approximately 0% and a false positive rate of 40%, roughly halving the number of times the entire cascade is evaluated.
					<br />
					<br />
					<span class="image fit"><img src="images/cascade.png"></span>
				</section>
			</article>		
		
						
		<!-- Testing Face Identification -->	
			<article class="container box style3">
				<header>
					<h2>Testing our face identifier</h2>
				</header>
				<section>
					<span class="image fit"><img src="images/plot_faces.png"></span>
					<hr />
					<p align="center">Great - looks like it's working! Let's try a few more</p>
					<span class="image fit"><img src="images/plot_faces2.png"></span>
				</section>
			</article>				

		<!-- Landmarks -->
			<article id="first" class="container box style1 right">
				<span class="image fit"><img src="images/landmarks.png"></span>
				<div class="inner">
					<header>
						<h2>2. Landmarks</h2>
					</header>
					<p>Now that we can identify faces within images we can use the <a href="http://dlib.net/" target="_blank">dlib</a> library to
					locate 68 facial landmarks. These are
					points on the face such as the corners of the mouth, along the eyebrows, on
					the eyes, and so forth.
					<br />
					Dlib includes <a href="http://dlib.net/face_landmark_detection.py.html" target="_blank">sample code here</a>.
					</p>
				</div>
			</article>

		<!-- Testing Landmarks -->	
			<article class="container box style3">
				<header>
					<h2>Testing our landmark locator</h2>
				</header>
				<section>
					<span class="image fit"><img src="images/landmarks_test3.png"></span>
					<hr />
					<p align="center">We can extract the landmarks as an array of <font color="red">x, y</font> coordinates for each face.</p>
					<span class="image fit"><img src="images/landmarks_test4.png"></span>
				</section>
			</article>				

		<!-- Distances and Ratios -->	
			<article class="container box style3">
				<header>
					<h2>3. Distances and Ratios</h2>
				</header>
				<section>
					<p>Now that we can extract the landmark coordinates we can calculate the distances between selected points.
					<br />
					<br />
					Unfortunately the distances aren't very useful as they will vary simply due to different image sizes. However the
					ratios between the distances will not change for a specific face regardless of the image size (providing the 
					aspect ratio of the image is consistent). 
					<br />
					The ratios are therefore what we want to calculate.
					<br />
					<br />
					Given we have 68 landmarks and we need 2 landmarks to return a distance. We can use the binomial coefficient formula to calculate
					the number of distances available from all 68 points:
					</p>
					<p style="text-align:center;">
					<img src="https://latex.codecogs.com/gif.latex?\binom{n}{k}=\frac{n!}{k!(n-k)!}&space;\\&space;\\&space;n&space;=&space;total&space;\:&space;points&space;\\&space;k&space;=&space;points&space;\:&space;selected" 
					title="\binom{n}{k}=\frac{n!}{k!(n-k)!} \\ \\ n = total \: points \\ k = points \: selected"></p>
					<p>Substituting n=68 and k=2 we will have 2,278 distances.
					<br />
					<br />
					We need 2 distances for 1 ratio so using the binomial coefficient formula again with n=2,278 and k=2 we can calculate 2,593,503 ratios! 
					<br />
					<br />
					We could use all of these ratios as our features but it's unlikely we will get that much additional info from all of them. Instead
					we will select 21 of the 68 landmarks. This will give us 210 distances and 21,945 ratios to use as features in our models.
					<br />
						<p style="text-align:center;"><img src="images/selected_points.png"></p>
					</p>
				</section>
			</article>	

		<!-- PCA -->	
			<article class="container box style3">
				<header>
					<h2>4. Principle Component Analysis</h2>
				</header>
				<section>
					<p>From the previous section we now have 21,945 ratios as our features. One of the issues with a large number of features is the
					<a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" target="_blank">curse of dimensionality</a>. In order to reduce the number of dimensions we can use 
					<a href="https://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank">Principle Component Analysis (PCA)</a>.
					</p>
					<p>PCA involves an <a href=https://en.wikipedia.org/wiki/Orthogonal_transformation target="_blank">orthogonal transformation</a> of your dimensions to emphasize variance. The goal of PCA 
					is to reduce the number of dimensions used for modelling to a suitable number that covers the majority of the variance in the data. 
					This is easily understood with some excellent visuals from <a href=http://setosa.io/ev/principal-component-analysis/ target="_blank">this site</a>.
					</p>
					
					<p>In the below 2d example we have 5 data points and 2 features (x & y). Each datapoint has both an x value and a y value which we would use in our models.
					Using PCA we can create a transform the axis into pc1 and pc2.
					</p>
					<p style="text-align:center;"><img src="images/pca_2d.png"></p>
					<p>We haven't changed anything about the data. Both pc1 and pc2 axis are a linear combination of x & y. But as you can see from the below pc1 now gives as a lot of of information about the points
					whilst pc2 gives very little and can be ignored. We have therefore reduced our dimensions from 2 to 1 and can use only pc1 in our model.
					</p>
					<p style="text-align:center;"><img src="images/pca_2d_footer.png"></p>
					<p>Below is a similar example using 3 dimensions.
					</p>
					<p style="text-align:center;"><img src="images/pca_3d.gif"></p>
					<p>Using PCA we can reduce our 21,945 features 
					to 150 which gives us 99.98% of the variance - and from the graph you can see that using as few as 50 features will give you over 98% of the variance.
					</p>
					<p style="text-align:center;"><img src="images/pca.png"></p>
				</section>
			</article>	
			
			
		<!-- Problem 1 -->	
			<article class="container box style3">
				<header>
					<h2>Problem 1: Facial Recognition</h2>
				</header>
				<section>
					<p>For our first 2 classification models I scraped approximately 60-100 images (total 1,938) of 25 different celebrities.
					</p>
					<p>
					Other than removing images showing multiple people I did not filter or adjust any images. We therefore have images with faces that are not frontally aligned and we would expect these to perform poorly
					in our models given no adjustments to the ratio calculations.
					</p>
					<p style="text-align:center;"><img src="images/photo-collage.png"></p>
					<p>
					Next we split our data into 80% for training and 20% for testing and fit the training data to 6 classification models.
					We then use these models to predict the celebrities in our test data. The results are summarized below:
					</p>
									
					<div class="table-wrapper">
						<table class="default">
							<thead>
								<tr>
									<th>Model</th>
									<th>Precision</th>
									<th>Recall</th>
									<th>F1-score</th>
									<th>Support</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>Logistic Regression</td>
									<td>0.88</td>
									<td>0.88</td>
									<td>0.87</td>
									<td>388</td>
								</tr>
								<tr>
									<td>Support Vector</td>
									<td>0.83</td>
									<td>0.82</td>
									<td>0.82</td>
									<td>388</td>
								</tr>
								<tr>
									<td>Naive Bayes</td>
									<td>0.69</td>
									<td>0.61</td>
									<td>0.61</td>
									<td>388</td>
								</tr>
								<tr>
									<td>Nearest Neighbors</td>
									<td>0.51</td>
									<td>0.49</td>
									<td>0.48</td>
									<td>388</td>
								</tr>
								<tr>
									<td>Decision Tree</td>
									<td>0.41</td>
									<td>0.39</td>
									<td>0.39</td>
									<td>388</td>
								</tr>
								<tr>
									<td>Random Forest</td>
									<td>0.48</td>
									<td>0.46</td>
									<td>0.46</td>
									<td>388</td>
								</tr>
							</tbody>
						</table>
					</div>
					<hr />
					<p>Using gridsearch to test different model parameters we are able to improve the Support Vector results:
					</p>
					<div class="table-wrapper">
						<table class="default">
							<thead>
								<tr>
									<th>Model</th>
									<th>Precision</th>
									<th>Recall</th>
									<th>F1-score</th>
									<th>Support</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>Support Vector</td>
									<td>0.88</td>
									<td>0.88</td>
									<td>0.88</td>
									<td>388</td>
								</tr>
							</tbody>
						</table>
					</div>
					<hr />
					<p>And here is our classification report:
					<div class="table-wrapper">
						<table class="default">
							<thead>
								<tr>
									<th>Model</th>
									<th>Precision</th>
									<th>Recall</th>
									<th>F1-score</th>
									<th>Support</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>Angelina Jolie</td>
									<td>0.89</td>
									<td>0.77</td>
									<td>0.83</td>
									<td>22</td>
								</tr>
								<tr>
									<td>Brad Pitt</td>
									<td>0.86</td>
									<td>0.83</td>
									<td>0.84</td>
									<td>23</td>
								</tr>
								<tr>
									<td>George Clooney</td>
									<td>0.89</td>
									<td>0.84</td>
									<td>0.86</td>
									<td>19</td>
								</tr>
								<tr>
									<td>Jennifer Lawrence</td>
									<td>0.88</td>
									<td>0.93</td>
									<td>0.90</td>
									<td>15</td>
								</tr>
								<tr>
									<td>Jessica Alba</td>
									<td>0.94</td>
									<td>0.89</td>
									<td>0.91</td>
									<td>18</td>
								</tr>
								<tr>
									<td>John Travolta</td>
									<td>0.87</td>
									<td>0.93</td>
									<td>0.90</td>
									<td>14</td>
								</tr>
								<tr>
									<td>Lucy Liu</td>
									<td>0.80</td>
									<td>1.00</td>
									<td>0.89</td>
									<td>12</td>
								</tr>
								<tr>
									<td>Margot Robbie</td>
									<td>0.64</td>
									<td>0.78</td>
									<td>0.70</td>
									<td>9</td>
								</tr>
								<tr>
									<td>Ryan Reynolds</td>
									<td>0.86</td>
									<td>0.67</td>
									<td>0.75</td>
									<td>18</td>
								</tr>
								<tr>
									<td>Scarlett Johansson</td>
									<td>0.81</td>
									<td>0.81</td>
									<td>0.81</td>
									<td>16</td>
								</tr>
								<tr>
									<td>Anthony Hopkins</td>
									<td>0.90</td>
									<td>0.69</td>
									<td>0.78</td>
									<td>13</td>
								</tr>
								<tr>
									<td>Cameron Diaz</td>
									<td>1.00</td>
									<td>1.00</td>
									<td>1.00</td>
									<td>17</td>
								</tr>
								<tr>
									<td>Denzel Washington</td>
									<td>1.00</td>
									<td>0.96</td>
									<td>0.98</td>
									<td>25</td>
								</tr>
								<tr>
									<td>Emma Stone</td>
									<td>1.00</td>
									<td>0.93</td>
									<td>0.96</td>
									<td>14</td>
								</tr>
								<tr>
									<td>Emma Watson</td>
									<td>0.79</td>
									<td>0.85</td>
									<td>0.81</td>
									<td>13</td>
								</tr>
								<tr>
									<td>Keanu Reeves</td>
									<td>0.81</td>
									<td>1.00</td>
									<td>0.90</td>
									<td>13</td>
								</tr>
								<tr>
									<td>Kevin Spacey</td>
									<td>0.95</td>
									<td>0.95</td>
									<td>0.95</td>
									<td>20</td>
								</tr>
								<tr>
									<td>Kristen Stewart</td>
									<td>0.91</td>
									<td>0.91</td>
									<td>0.91</td>
									<td>11</td>
								</tr>
								<tr>
									<td>Leonardo Dicaprio</td>
									<td>0.91</td>
									<td>0.91</td>
									<td>0.91</td>
									<td>11</td>
								</tr>
								<tr>
									<td>Matt Damon</td>
									<td>0.86</td>
									<td>0.75</td>
									<td>0.8</td>
									<td>16</td>
								</tr>
								<tr>
									<td>Mila Kunis</td>
									<td>0.80</td>
									<td>0.92</td>
									<td>0.86</td>
									<td>13</td>
								</tr>
								<tr>
									<td>Oprah Winfrey</td>
									<td>0.95</td>
									<td>1.00</td>
									<td>0.97</td>
									<td>19</td>
								</tr>
								<tr>
									<td>Tom Cruise</td>
									<td>0.78</td>
									<td>0.93</td>
									<td>0.85</td>
									<td>15</td>
								</tr>
								<tr>
									<td>Tom Hardy</td>
									<td>0.85</td>
									<td>0.79</td>
									<td>0.81</td>
									<td>14</td>
								</tr>
								<tr>
									<td>Will Smith</td>
									<td>0.89</td>
									<td>1.00</td>
									<td>0.94</td>
									<td>8</td>
								</tr>
								<tr>
									<td><strong>Avg/Total</strong></td>
									<td><strong>0.88</strong></td>
									<td><strong>0.88</strong></td>
									<td><strong>0.88</strong></td>
									<td><strong>388</strong></td>
								</tr>
							</tbody>
						</table>
					</div>
				</section>
			</article>			
			
		<!-- Problem 2 -->	
			<article class="container box style3">
				<header>
					<h2>Problem 2: Gender Classification</h2>
				</header>
				<section>
					<p>
					For Problem 2 we will be using the 
					<a href=http://faculty.chicagobooth.edu/bernd.wittenbrink/cfd/index.html target="_blank">Chicago Face Database</a>
					<span class="image"><img src="images/cfd.png"></span>
					</p>
					<p>
					The Chicago Face Database was developed at the University of Chicago by Debbie S. Ma, Joshua Correll, and Bernd Wittenbrink. 
					The CFD is intended for use in scientific research. It provides high-resolution, standardized photographs of male and female faces of varying ethnicity between the ages of 17-65. 
					These data include both physical attributes (e.g., face size) as well as subjective ratings by independent judges (e.g., attractiveness).
					</p>
					<p>From this dataset we have <strong>287 male</strong> and <strong>305 female</strong> images. I used an 80% train/20% test split again.
					</p>
					<div class="table-wrapper">
						<table class="default">
							<thead>
								<tr>
									<th>Model</th>
									<th>Precision</th>
									<th>Recall</th>
									<th>F1-score</th>
									<th>Support</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>Logistic Regression</td>
									<td>0.81</td>
									<td>0.81</td>
									<td>0.81</td>
									<td>119</td>
								</tr>
								<tr>
									<td>Support Vector</td>
									<td>0.79</td>
									<td>0.79</td>
									<td>0.79</td>
									<td>119</td>
								</tr>
								<tr>
									<td>Naive Bayes</td>
									<td>0.58</td>
									<td>0.59</td>
									<td>0.58</td>
									<td>119</td>
								</tr>
								<tr>
									<td>Nearest Neighbors</td>
									<td>0.71</td>
									<td>0.71</td>
									<td>0.71</td>
									<td>119</td>
								</tr>
								<tr>
									<td>Decision Tree</td>
									<td>0.64</td>
									<td>0.61</td>
									<td>0.61</td>
									<td>119</td>
								</tr>
								<tr>
									<td>Random Forest</td>
									<td>0.63</td>
									<td>0.63</td>
									<td>0.63</td>
									<td>119</td>
								</tr>
							</tbody>
						</table>
					</div>
					<hr />
					<p>Classification report:
					</p>
										<div class="table-wrapper">
						<table class="default">
							<thead>
								<tr>
									<th>Model</th>
									<th>Precision</th>
									<th>Recall</th>
									<th>F1-score</th>
									<th>Support</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>Female</td>
									<td>0.85</td>
									<td>0.81</td>
									<td>0.83</td>
									<td>69</td>
								</tr>
								<tr>
									<td>Male</td>
									<td>0.75</td>
									<td>0.80</td>
									<td>0.78</td>
									<td>50</td>
								</tr>
								<tr>
									<td><strong>Avg/Total</strong></td>
									<td><strong>0.81</strong></td>
									<td><strong>0.81</strong></td>
									<td><strong>0.81</strong></td>
									<td><strong>119</strong></td>
								</tr>
							</tbody>
						</table>
					</div>
				</section>
			</article>			

		<!-- Problem 3 -->	
			<article class="container box style3">
				<header>
					<h2>Problem 3: Race Classification</h2>
				</header>
				<section>
					<p>
					For Problem 3 we will be using the 
					<a href=http://faculty.chicagobooth.edu/bernd.wittenbrink/cfd/index.html target="_blank">Chicago Face Database</a> again.
					<span class="image"><img src="images/cfd.png"></span>
					</p>
					<p>The dataset includes 4 race labels:
					<strong>Asian (109), Black (192), Latino (108), White (183) </strong>.
					</p>
					<p>
					I continued using an 80% train/20% test split.
					</p>
					<div class="table-wrapper">
						<table class="default">
							<thead>
								<tr>
									<th>Model</th>
									<th>Precision</th>
									<th>Recall</th>
									<th>F1-score</th>
									<th>Support</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>Logistic Regression</td>
									<td>0.73</td>
									<td>0.75</td>
									<td>0.73</td>
									<td>119</td>
								</tr>
								<tr>
									<td>Support Vector</td>
									<td>0.74</td>
									<td>0.76</td>
									<td>0.75</td>
									<td>119</td>
								</tr>
								<tr>
									<td>Naive Bayes</td>
									<td>0.63</td>
									<td>0.59</td>
									<td>0.59</td>
									<td>119</td>
								</tr>
								<tr>
									<td>Nearest Neighbors</td>
									<td>0.70</td>
									<td>0.71</td>
									<td>0.70</td>
									<td>119</td>
								</tr>
								<tr>
									<td>Decision Tree</td>
									<td>0.57</td>
									<td>0.56</td>
									<td>0.56</td>
									<td>119</td>
								</tr>
								<tr>
									<td>Random Forest</td>
									<td>0.55</td>
									<td>0.57</td>
									<td>0.55</td>
									<td>119</td>
								</tr>
							</tbody>
						</table>
					</div>
					<hr />
					<p>Using gridsearch to test different model parameters we are able to improve the Logistic Regression results:
					</p>
					<div class="table-wrapper">
						<table class="default">
							<thead>
								<tr>
									<th>Model</th>
									<th>Precision</th>
									<th>Recall</th>
									<th>F1-score</th>
									<th>Support</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>Support Vector</td>
									<td>0.81</td>
									<td>0.82</td>
									<td>0.82</td>
									<td>119</td>
								</tr>
							</tbody>
						</table>
					</div>
					<hr />
					<p>Classification report:
					<div class="table-wrapper">
						<table class="default">
							<thead>
								<tr>
									<th>Model</th>
									<th>Precision</th>
									<th>Recall</th>
									<th>F1-score</th>
									<th>Support</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>Asian</td>
									<td>0.85</td>
									<td>0.81</td>
									<td>0.83</td>
									<td>21</td>
								</tr>
								<tr>
									<td>Black</td>
									<td>0.92</td>
									<td>0.97</td>
									<td>0.94</td>
									<td>34</td>
								</tr>
								<tr>
									<td>Latino</td>
									<td>0.63</td>
									<td>0.50</td>
									<td>0.56</td>
									<td>24</td>
								</tr>
								<tr>
									<td>White</td>
									<td>0.82</td>
									<td>0.90</td>
									<td>0.86</td>
									<td>40</td>
								</tr>
								<tr>
									<td><strong>Avg/Total</strong></td>
									<td><strong>0.81</strong></td>
									<td><strong>0.82</strong></td>
									<td><strong>0.82</strong></td>
									<td><strong>119</strong></td>
								</tr>
							</tbody>
						</table>
					</div>
				</section>
			</article>

		<!-- Problem 4 -->	
			<article class="container box style3">
				<header>
					<h2>Problem 4: Age Regression</h2>
				</header>
				<section>
					<p>
					For Problem 4 we will continue with the 
					<a href=http://faculty.chicagobooth.edu/bernd.wittenbrink/cfd/index.html target="_blank">Chicago Face Database</a>.
					<span class="image"><img src="images/cfd.png"></span>
					</p>
					<p>The dataset includes an age estimation of the face from 26 raters with ages between 28 and 56 years old.
					</p>
					<p>
					Using the same 80% train/20% test split we have the below scoring:
					</p>
					<div class="table-wrapper">
						<table class="default">
							<thead>
								<tr>
									<th>Model</th>
									<th>Mean Absolute Error</th>
									<th>Mean Squared Error</th>
									<th>R2 Score</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>Linear</td>
									<td>4.83</td>
									<td>44.42</td>
									<td>-0.06</td>
								</tr>
								<tr>
									<td>Ridge</td>
									<td>4.51</td>
									<td>37.22</td>
									<td>0.11</td>
								</tr>
								<tr>
									<td>Lasso</td>
									<td>4.45</td>
									<td>33.26</td>
									<td>0.21</td>
								</tr>
								<tr>
									<td>Elastic</td>
									<td>4.42</td>
									<td>32.62</td>
									<td>0.22</td>
								</tr>
								<tr>
									<td>Bayes</td>
									<td>4.42</td>
									<td>32.72</td>
									<td>0.22</td>
								</tr>
								<tr>
									<td>Support Vector</td>
									<td>4.22</td>
									<td>31.91</td>
									<td>0.25</td>
								</tr>
							</tbody>
						</table>
					</div>
					<hr />
					<p style="text-align:center;"><img src="images/age.png"></p>
				</section>
			</article>
		
		<!-- Problem 5 -->	
			<article class="container box style3">
				<header>
					<h2>Problem 5: Attractiveness Regression</h2>
				</header>
				<section>
					<p>For our final problem we are using the <a href="http://www.hcii-lab.net/data/SCUT-FBP/EN/introduce.html" target="_blank">SCUT-FBP</a> dataset together with the
					<a href=http://faculty.chicagobooth.edu/bernd.wittenbrink/cfd/index.html target="_blank">Chicago Face Database</a> from the earlier problems.
					</p>
					<p>
					A novel face dataset with attractiveness ratings, namely the SCUT-FBP dataset(A dataset for facial beauty perception), is developed for automatic facial beauty perception in this work. 
					The dataset provides a benchmark to evaluate the performance of different methods for facial attractiveness prediction. The SCUT-FBP dataset contains 500 different Asian female subjects with attractiveness ratings, 
					all of which have been verified in terms of rating distribution, standard deviation, consistency, and self-consistency.
					</p>
					<p style="text-align:center;"><img src="images/SCUT.png"></p>
					<p>Both datasets provide perceived attractiveness ratings on a scale of 1 to 5.
					</p>
					<p>
					Using the same 80% train/20% test split we have the below scoring:
					</p>
					<div class="table-wrapper">
						<table class="default">
							<thead>
								<tr>
									<th>Model</th>
									<th>Mean Absolute Error</th>
									<th>Mean Squared Error</th>
									<th>R2 Score</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>Linear</td>
									<td>0.54</td>
									<td>0.48</td>
									<td>0.17</td>
								</tr>
								<tr>
									<td>Ridge</td>
									<td>0.53</td>
									<td>0.46</td>
									<td>0.20</td>
								</tr>
								<tr>
									<td>Lasso</td>
									<td>0.60</td>
									<td>0.55</td>
									<td>0.04</td>
								</tr>
								<tr>
									<td>Elastic Net</td>
									<td>0.57</td>
									<td>0.50</td>
									<td>0.12</td>
								</tr>
								<tr>
									<td>Bayes</td>
									<td>0.51</td>
									<td>0.43</td>
									<td>0.25</td>
								</tr>
								<tr>
									<td>Support Vector</td>
									<td>0.65</td>
									<td>1.12</td>
									<td>-0.54</td>
								</tr>
							</tbody>
						</table>
					</div>
					<hr />
					<p style="text-align:center;"><img src="images/attractive.png"></p>
				</section>
			</article>
	
		<!-- Problem 5 -->	
			<article class="container box style3">
				<header>
					<h2>Conclusion</h2>
				</header>
				<section>
					<p>Overall I am happy with the results. Facial Ratios are effective as features for Facial Recognition and Gender Classification. They also worked well
					for Race classification.
					</p>
					<p>As expected they did not perform very well with regard to both age regression and attractiveness regression. As the labels for these were subjective estimates to start with
					we would likely need more data and more features to improve the results.
					</p>
				<header>
					<h2>Next Steps</h2>
				</header>
					<p>The first step to improving our results would be to transform our images by adjusting for different facial poses and alignment. This would improve the calculation of ratios and
					we would expect our model performance to improve.
					</p>
					<p>To further improve our models (in particular age / attractiveness regressions) we would need to incorporate new features to give us additional information.
					</p>
				</section>
			</article>

		-->

		<section id="footer">
			<ul class="icons">
				<li><a href="#" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
				<li><a href="#" class="icon fa-facebook"><span class="label">Facebook</span></a></li>
				<li><a href="#" class="icon fa-google-plus"><span class="label">Google+</span></a></li>
				<li><a href="#" class="icon fa-pinterest"><span class="label">Pinterest</span></a></li>
				<li><a href="#" class="icon fa-dribbble"><span class="label">Dribbble</span></a></li>
				<li><a href="#" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
			</ul>
			<div class="copyright">
				<ul class="menu">
					<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
				</ul>
			</div>
		</section>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.poptrox.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

			
			<script type="text/javascript">
			// create the back to top button
			$('body').prepend('<a href="#" class="back-to-top">Back to Top</a>');

			var amountScrolled = 300;

			$(window).scroll(function() {
				if ( $(window).scrollTop() > amountScrolled ) {
					$('a.back-to-top').fadeIn('slow');
				} else {
					$('a.back-to-top').fadeOut('slow');
				}
			});

			$('a.back-to-top, a.simple-back-to-top').click(function() {
				$('html, body').animate({
					scrollTop: 0
				}, 700);
				return false;
			});
			</script>
						
	</body>
</html>